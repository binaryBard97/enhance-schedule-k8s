The paper you uploaded, titled "Singularity: Planet-Scale, Preemptive and Elastic Scheduling of AI Workloads," focuses on the development of "Singularity," a distributed scheduling service by Microsoft designed to efficiently manage deep learning workloads across a global fleet of AI accelerators like GPUs and FPGAs.

### Key Points of the Paper:

1. **Purpose of Singularity**:
   - Singularity is developed to maximize the utilization of AI accelerators on a global scale, thereby reducing costs for cloud providers.
   - It achieves this by making AI workloads preemptible, migratable, and dynamically resizable.

2. **Core Mechanisms**:
   - **Preemption and Migration**: Singularity can checkpoint, preempt, and migrate AI workloads transparently, ensuring that jobs can be moved across nodes, clusters, or regions without losing progress.
   - **Resizing/Elasticity**: The service can elastically scale jobs up or down in response to available resources, again, without requiring changes to the user's code.

3. **Technical Innovations**:
   - **Device Proxy**: This mechanism decouples job execution from the underlying hardware, enabling transparent migration and resizing. It uses a technique called "replica splicing" to allow multiple jobs to time-share the same hardware resources with minimal overhead.
   - **Distributed Barrier**: A novel synchronization mechanism that ensures consistent checkpoints across distributed jobs, allowing them to resume precisely from where they were preempted.

4. **Evaluation**:
   - The paper presents experiments demonstrating that Singularity's mechanisms result in negligible performance overhead (within 3%) while providing significant efficiency and reliability improvements.

5. **Impact**:
   - Singularity enables more efficient use of AI accelerators on a global scale, improving fault tolerance, allowing opportunistic use of spare capacity, and supporting flexible job management without user intervention.

The paper is structured with an introduction, details on the core scheduling mechanisms, an overview of the device-proxy approach, and sections dedicated to the design of transparent migration and elasticity. It also includes an implementation section and evaluation results.

If you need a more detailed analysis or specific sections summarized, please let me know!

Singularity aims to address the challenge of maximizing the utilization of AI hardware accelerators (like GPUs) across Microsoft's global infrastructure, to reduce costs and increase throughput for AI workloads. It does this by ensuring no resources are idled, offering job-level Service Level Agreements (SLAs), and providing resilience against failures. The problem it solves is maintaining high efficiency and reliability in handling deep learning workloads that can vary in size, require different priorities (e.g., inference vs. training), and must dynamically scale up or down depending on availability.

To simulate this with the concept of Kubernetes, consider how Kubernetes orchestrates containerized applications across clusters. Singularity functions similarly, but for AI workloads, by treating a global fleet of accelerators as a shared, unified resource. Just as Kubernetes schedules pods across nodes in a cluster, Singularity schedules AI jobs (such as DNN training) across global accelerators, optimizing for resource usage. Instead of Kubernetes managing container resources like CPU and memory, Singularity manages accelerators, preempting and migrating jobs to maximize efficiency while ensuring that job SLAs (similar to Kubernetes’ QoS classes) are met.


Singularity preempts and migrates jobs by utilizing two key mechanisms—**preemption** and **migration**—to maximize efficiency while maintaining the performance and correctness of AI workloads.

### 1. **Preemption and Migration Mechanism**
   - **Preemption**: Singularity can transparently interrupt (or "preempt") an AI job that is running on a set of accelerators, such as GPUs, and move it to another location (node, cluster, or region). This is done without losing any of the job's progress, thanks to the use of **checkpointing**.
     - **Checkpointing**: When a job is preempted, its current state (e.g., program counter, memory, and model parameters) is saved via a checkpoint. This allows the job to be resumed exactly where it left off, either on the same or different hardware, without having to restart the entire process.
   
   - **Migration**: After preemption, Singularity migrates the job to other available resources, either within the same data center or globally. Migration involves restoring the job's state from the checkpoint to a new set of accelerators and resuming execution seamlessly.

   These operations are **transparent** to the user, meaning that users do not need to modify their code or handle the complexities of preemption and migration themselves.

### 2. **When Would Preemption and Migration be Necessary?**
Preemption and migration are required in the following scenarios:

   - **Resource Optimization**: When there is a sudden spike in demand for high-priority jobs (e.g., inference tasks that require low latency), Singularity can preempt lower-priority tasks (like training jobs), free up resources, and migrate them to other nodes where capacity is available. This ensures the system is always working at its highest capacity.
   
   - **Elastic Scaling**: In cases where a workload suddenly requires more resources (e.g., increased demand for inference due to more users), Singularity can elastically scale down lower-priority jobs by preempting and migrating them to areas with spare capacity. This dynamic adjustment ensures efficient resource use while respecting job SLAs.
   
   - **Resilience to Failures**: If an accelerator or a set of nodes fails, Singularity preempts the jobs running on those nodes and migrates them to healthy nodes or clusters, minimizing the loss of work and avoiding a complete restart of long-running deep learning jobs.
   
   - **Cost Reduction and Fragmentation**: Similar to how Kubernetes evicts pods to defragment resources and optimize resource utilization, Singularity migrates smaller jobs or jobs with flexible SLAs to consolidate workloads on fewer nodes, thereby freeing up resources for larger, higher-priority jobs.

### 3. **Technical Steps in Preemption and Migration**
   - **Checkpointing**: During preemption, the job’s state (memory, device contexts, and parameters) is saved using a **distributed barrier mechanism** that ensures consistency across all nodes in a distributed job.
   - **Job Suspension**: The job is then suspended, and the system initiates migration by saving the job state.
   - **Migration**: The job state is transferred to another set of accelerators, possibly in a different region, and the job is resumed from the exact point of suspension.
   - **Load Redistribution**: The scheduler reassigns resources dynamically based on the global workload and hardware availability.

In summary, Singularity uses preemption and migration to maximize efficiency by ensuring that no resource is underutilized, all while maintaining stringent SLAs and adapting to fluctuating job loads across its global infrastructure

Kubernetes uses **preemption** and **migration** to improve resource utilization and efficiency, but not exactly in the same way or to the same extent as Singularity.

### 1. **Kubernetes Preemption**
   - **Preemption in Kubernetes** is primarily used to ensure that higher-priority workloads can be scheduled even when cluster resources are fully occupied by lower-priority workloads. When a high-priority pod cannot be scheduled due to a lack of available resources, Kubernetes preempts (evicts) lower-priority pods to make room.
     - **Priority and Preemption**: Kubernetes uses the concept of **PriorityClasses**, where workloads are assigned different priorities. If a high-priority pod needs to be scheduled, and there are no available resources, Kubernetes will evict lower-priority pods to free up those resources.
     - **Soft Evictions**: Preempted pods are not immediately terminated but are given a grace period to gracefully shut down, allowing them to save their state (if possible).

However, Kubernetes does not have native, built-in support for **checkpointing** and **resuming** jobs (as Singularity does), which means that when a pod is preempted, it often has to restart from scratch unless the application itself is designed to handle restarts. Kubernetes can preempt jobs, but it does not "migrate" them in the sense of seamlessly moving a running job from one node to another with its state intact.

### 2. **Kubernetes Migration**
   - **Migration in Kubernetes** is not a core feature for running workloads in the same way as Singularity. Kubernetes schedules new workloads on the most appropriate node, but once a pod is running, Kubernetes does not automatically "migrate" the pod to a different node if resources elsewhere become available.
     - **Node Draining**: When nodes are decommissioned or enter maintenance mode (through a process called **node draining**), Kubernetes will schedule pods on other nodes. However, the pods will not be migrated in their running state—they will be terminated and restarted on a new node.
     - **Stateful Workloads**: For stateful applications, Kubernetes provides features like **StatefulSets** and **Persistent Volumes** to help preserve data across pod restarts, but this still requires the pod to restart rather than seamlessly migrate.

### 3. **How Kubernetes Differs in Efficiency from Singularity**
   - **Pod Scheduling**: Kubernetes uses a scheduling algorithm that attempts to optimize resource use by selecting the most appropriate node to run a pod, considering resource availability, node health, and other factors like affinity rules. However, Kubernetes doesn’t guarantee maximum efficiency because it doesn’t preempt or migrate running pods based on real-time resource changes.
   - **Horizontal Scaling**: Kubernetes improves efficiency through features like **Horizontal Pod Autoscaling** (HPA), which can dynamically scale the number of pods in response to resource demand, but this is different from preemption or migration.
   - **Cluster Autoscaling**: Kubernetes can also scale the cluster itself by adding or removing nodes as needed to match workload demands, but it doesn’t involve moving live workloads across nodes.

### Summary
While Kubernetes uses preemption to prioritize workloads and has tools for scaling and scheduling to improve efficiency, it does **not** natively use migration or seamless preemption as Singularity does. In Singularity, preemption and migration are much more advanced because they involve **checkpointing, resuming jobs, and scaling** workloads dynamically without restarting them, leading to a more fine-grained and efficient use of global resources. Kubernetes improves efficiency in scheduling and scaling but lacks the stateful migration and real-time elasticity features that Singularity offers.




In Kubernetes, demonstrating the **lack of stateful migration and real-time elasticity** can be done by showing how pods behave during preemption and scaling. Specifically, we can show that when a lower-priority pod is preempted, it is terminated and restarted rather than migrated with its current state intact. Additionally, we can demonstrate that when scaling down or up, Kubernetes starts new pods rather than reallocating the workload in a way that preserves current execution.

### 1. **Priority Preemption Example**

The following YAML shows how Kubernetes handles preemption:

- Create two pods, one with low priority (`low-priority-pod`) and one with high priority (`high-priority-pod`). When resources are constrained, Kubernetes will preempt the low-priority pod to make room for the high-priority pod.

```yaml
# priorityclass.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000  # Higher value = higher priority
globalDefault: false
description: "High priority class"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 100  # Lower value = lower priority
globalDefault: false
description: "Low priority class"
```

- Deploy a low-priority pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: low-priority-pod
spec:
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - name: data
          mountPath: /usr/share/nginx/html
      command: ["/bin/sh", "-c"]
      args:
        - while true; do echo $(date) Writing data... >> /usr/share/nginx/html/data.txt;
          sleep 5;
          done
      resources:
        requests:
          memory: 1800Mi
          cpu: 1000m
  volumes:
    - name: data
      emptyDir: {}

```

- Deploy a high-priority pod that will preempt the low-priority pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high-priority-pod
spec:
  priorityClassName: high-priority
  containers:
    - name: nginx
      image: nginx
      resources:
        requests:
          memory: 1000Mi
          cpu: 500m
```

### Demonstration of Preemption
1. Create both pods in a constrained environment (e.g., limit available CPU/memory on the cluster).
2. Once the high-priority pod is deployed, Kubernetes will preempt the low-priority pod by terminating it.
3. **Key Point**: The low-priority pod **will not** be migrated to another node, and its state will be lost. You will notice that the file either does not exist or contains no previous data, proving that the state (written data) was lost because the pod’s filesystem is ephemeral and does not persist after termination.
To observe this, run:
```bash
kubectl apply -f priorityclass.yaml
kubectl apply -f low-priority-pod.yaml
kubectl apply -f high-priority-pod.yaml
kubectl get pods -w  # Watch the low-priority pod get preempted
```
Here, the terminated pods lose their state, and the data will only persist if it’s on a persistent volume. There's no real-time elasticity where resources are reallocated without terminating pods.

### Conclusion
These examples show that Kubernetes does **not** have real-time migration capabilities, and preempted or scaled-down pods lose their state and must restart from scratch.

Kubernetes **can** work with AI workloads like Deep Neural Network (DNN) training jobs, but there are **limitations** compared to specialized systems like Singularity, particularly when it comes to **long-running jobs** that need high resilience and **checkpointing**. Let's break this down in terms of what Kubernetes can do, its strengths, and where it falls short for DNN training jobs.

### 1. **Kubernetes Strengths for AI Workloads**:
   - **Scalability**: Kubernetes is great at scaling resources for AI workloads. You can spin up hundreds or thousands of pods across multiple nodes to handle distributed AI jobs. Autoscaling, both at the node and pod level, makes it possible to dynamically scale AI workloads based on resource usage or load.
   - **Containerization**: Kubernetes orchestrates containerized workloads, making it easier to package and deploy complex AI environments (such as those using TensorFlow, PyTorch, or custom frameworks). Containers can encapsulate all dependencies needed for AI models, ensuring consistency across different environments.
   - **Hardware Acceleration**: Kubernetes can manage workloads that leverage **GPUs** and other hardware accelerators, which are crucial for DNN training jobs. By configuring the cluster to support GPU scheduling (using tools like NVIDIA's Kubernetes Device Plugin), you can ensure that AI workloads have access to the necessary hardware.
   - **Data Handling**: Kubernetes supports **persistent volumes** and **object storage** integrations (like S3 or GCP Storage) for storing large datasets, model checkpoints, and outputs. This is critical for DNN training jobs that rely on massive amounts of data.
   - **Distributed Training**: Kubernetes works well with distributed AI frameworks like **Kubeflow** or **Horovod**. These frameworks enable multi-node distributed training, allowing you to train large models efficiently across multiple GPUs or nodes.

### 2. **Kubernetes Limitations for Long-Running AI Jobs (e.g., DNN Training Jobs)**:
   - **No Native Checkpointing**: Kubernetes does **not natively support checkpointing** of running jobs. If a DNN training job is running for several hours or days and the pod gets terminated (due to node failure, preemption, or autoscaling), the job must **restart from scratch**. This can be costly, especially for long-running jobs.
     - **Workaround**: Developers need to manually implement checkpointing at the **application level** (e.g., using TensorFlow or PyTorch's model checkpointing features). However, Kubernetes itself won't automatically checkpoint and migrate the job like Singularity does.
   
   - **No Live Migration**: Unlike Singularity, Kubernetes does not support **live migration** of running pods. If a node fails or is drained for maintenance, Kubernetes will terminate the pod and reschedule it on another node, but the job will have to **restart from the beginning** unless manual checkpointing is in place.
     - In DNN training jobs, where each epoch might take hours, this is a major limitation.
   
   - **Preemption Without State Preservation**: Kubernetes can preempt lower-priority jobs in favor of higher-priority ones, but preempted jobs are simply terminated. The system doesn't automatically resume a job from its last state, which is problematic for long-running training jobs.
   
   - **GPU Utilization**: While Kubernetes can manage GPUs, maximizing GPU utilization for DNN training jobs requires fine-tuning. Out-of-the-box, Kubernetes might not ensure that GPUs are fully utilized for tasks like training, leading to potential inefficiencies unless properly configured.

### 3. **Kubernetes Can Be Made More Suitable for Long-Running AI Jobs with Add-ons**:
   - **Kubeflow**: To make Kubernetes more suitable for AI workloads, **Kubeflow** (an open-source Kubernetes-native platform) can be used. Kubeflow makes it easier to run distributed AI training and offers features like distributed job orchestration and hyperparameter tuning. However, even Kubeflow lacks **native checkpointing** for fault-tolerant job resumption.
   
   - **Argo Workflows**: For long-running workflows, **Argo Workflows** can help orchestrate complex, multi-step pipelines. It can help automate retries or manage jobs but doesn’t solve the lack of live migration or checkpointing.
   
   - **Persistent Storage**: Using persistent volumes for checkpointing models can mitigate some issues. For instance, you can store periodic checkpoints in external storage (e.g., NFS, S3, or GCS) and restart jobs from the last checkpoint in case of failure. But this is still **manual** and not as seamless as in Singularity.

### 4. **When Kubernetes is Not Ideal for AI Workloads**:
   - **Long-Running Jobs with High Failure Costs**: If you have long-running AI training jobs that take days or weeks and where restarting from scratch would be **prohibitively expensive**, Kubernetes without native checkpointing and live migration isn’t the best fit.
   - **Highly Elastic Workloads**: AI workloads that require **dynamic scaling** based on real-time resource availability (e.g., scaling down, migrating to different regions, or reallocating GPUs dynamically) are better handled by systems like Singularity.

### Conclusion:
Kubernetes can certainly **handle AI workloads** like DNN training jobs, but it lacks some critical features such as **native checkpointing**, **live migration**, and **stateful preemption**, which are essential for long-running jobs that must maintain resilience against failures. To make Kubernetes work for these scenarios, you'll need to integrate manual checkpointing and potentially use AI-specific orchestration layers like Kubeflow.

If you need **maximum efficiency and fault tolerance** for long-running jobs without restarting from scratch upon failure, Kubernetes alone is not ideal. A system like **Singularity**, which is designed for large-scale AI workloads and supports live migration and checkpointing, would be better suited for such scenarios.